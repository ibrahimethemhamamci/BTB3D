optimizer:
  beta1: 0.0
  beta2: 0.99
  epsilon: 1e-8
  grad_accum_size: 1

lr_scheduler:
  learning_rate: 1e-5
  gen_lr_scale: 1.0
  disc_lr_scale: 0.8
  warmup_steps: 500

data:
  train_dir: /path/to/your/data # a text file, one path per line, refer to video path. eg. /aaa/1.mkv
  valid_dir: /path/to/your/data
  train_batch_size: 1
  valid_batch_size: 1
  num_workers: 32
  num_epochs: 10
  spatial_size: 512
  num_frames: 73

logging:
  tensorboard_dir: ./exps/tb
  refresh_codebook_tracker_steps: 100
  validate_every_step: 500


io:
  ckpt_base_dir: ./exps/ckpts
  output_base_dir: ./exps/outputs

ema:
  apply_ema: false
  decay_rate: 0.999

gan:
  use_gan: true
  use_lecam_ema: true
  lecam_loss_weight: 0.001
  g_adversarial_loss_weight: 0.1
  d_adversarial_loss_weight: 1.0
  gradient_penalty_cost: 10.0
  generator_loss_type: non-saturating
  discriminator_loss_type: non-saturating
  apply_gradient_penalty: true
  apply_gradient_penalty_every: 20

quantizer:
  commitment_cost: 0.25
  aux_loss_weight: 1.0
  entropy_loss_weight: 0.1
  entropy_loss_scale_factor: 3.0
  entropy_loss_decay_steps: 2000
  diversity_gamma: 1.0
  use_distributed_batch_entropy: true

perceptual:
  use_perceptual: false
  ckpt_path: vgg16-397923af.pth # resnet ckpt path
  perceptual_loss_weight: 0.0

checkpointing:
  inflate_from_2d: false
  inflate_ckpt_path:      # inflate 2d-> 3d ckpt path
  pretrained: /anvme/workspace/b180dc42-ct-rate/clip_maisi/O2-MAGVIT2/exps/ckpts/train_wavelettransform_9frames_16node_8x8x8_new_thirdstage/iter_12000/model.fixed.safetensors #load model.safetensors to init model weights only, otherwise load from a checkpoint dir save by accelerate
  continue_training: false # set to true if resume all states from previous checkpoint, set to false to only load model checkpoint
  checkpoint_every_step: 1000

max_grad_norm: 0.5 # clip grad if not null
recon_loss_weight: 5.0
modal: video
exp_name: train_wavelettransform_9frames_16node_8x8x8_new_preprocessedtune
